{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466fb799",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c141a822",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pooya/w/DroughtMonitoringIran/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import sqlite3\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import GradientBoostingClassifier # Changed from RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27845aff",
   "metadata": {},
   "source": [
    "# Load Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1361a6af",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('/home/pooya/w/DroughtMonitoringIran/')\n",
    "\n",
    "DATABASE_PATH = \"./database/database.db\"\n",
    "\n",
    "conn = sqlite3.connect(DATABASE_PATH)\n",
    "\n",
    "data = pd.read_sql(sql='SELECT * FROM data', con=conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94899866",
   "metadata": {},
   "source": [
    "# Select Columns and Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0de84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Drought Index and Period\n",
    "di = 'SPEI'\n",
    "di_scale = 3\n",
    "\n",
    "# Select Stations\n",
    "selected_stations = ['Ramsar', 'Nowshahr', 'Siahbisheh', 'Kiyasar', 'Kojur', 'Baladeh', 'Alasht', 'Babolsar', 'Gharakhil', 'Sari', 'Sari (dasht-e-naz airport)', 'Galugah', 'Bandar-e-amirabad', 'Amol', 'Polsefid']\n",
    "# selected_stations = ['Ramsar', 'Nowshahr']\n",
    "# selected_stations = ['Siahbisheh', 'Kiyasar', 'Kojur', 'Baladeh', 'Alasht']\n",
    "# selected_stations = ['Babolsar', 'Gharakhil', 'Sari', 'Sari (dasht-e-naz airport)', 'Galugah', 'Bandar-e-amirabad', 'Amol', 'Polsefid']\n",
    "\n",
    "# Select Columns\n",
    "selected_columns = [\n",
    "    'Station_Name', 'Station_ID',\n",
    "    'Station_Latitude', 'Station_Longitude', 'Station_Elevation',\n",
    "    'Date',\n",
    "    f'{di}_{di_scale}',\n",
    "    f'GPM_{di}_{di_scale}',\n",
    "    f'ERA5_{di}_{di_scale}',\n",
    "    'ERA5_Precipitation',\n",
    "    'GPM_Precipitation',\n",
    "    'TRMM_Precipitation',\n",
    "    'TERRACLIMATE_Precipitation',\n",
    "    'PERSIANNCDR_Precipitation',\n",
    "    'PET_MOD16A2GF',\n",
    "    'NDVI', 'EVI',\n",
    "    'LSTDay', 'LSTNight', 'LST',\n",
    "    'PCI_GPM', 'PCI_ERA5',\n",
    "    'VCI', 'TCI', 'VHI',\n",
    "    'CI_GPM', 'CI_ERA5',\n",
    "]\n",
    "\n",
    "# Select Start and End Date\n",
    "start_date = '2006-09'\n",
    "end_date = '2023-10'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c773c79b",
   "metadata": {},
   "source": [
    "# Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf78173",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data\\\n",
    "    .filter(items=selected_columns)\\\n",
    "        .query(\"Station_Name in @selected_stations and Date >= @start_date and Date < @end_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b1f34",
   "metadata": {},
   "source": [
    "# Add Some Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3303aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date, Year, Month\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m')\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "\n",
    "# SPI and SPEI Class\n",
    "df[f'{di}_{di_scale}_Class'] = pd.cut(df[f'{di}_{di_scale}'], bins=[-10, -2, -1.5, -1, 1, 1.5, 2, 10], labels=['ED', 'SD', 'MD', 'NN', 'MW', 'VW', 'EW'])\n",
    "\n",
    "# Month Sin & Cos\n",
    "df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "        \n",
    "# LST Diff\n",
    "df['LST_Diff'] = df['LSTDay'] - df['LSTNight']\n",
    "\n",
    "# Convert to Category\n",
    "df['Station_ID'] = df['Station_ID'].astype('category')\n",
    "df['Year'] = df['Year'].astype('category')\n",
    "df['Month'] = df['Month'].astype('category')\n",
    "df[f'{di}_{di_scale}_Class'] = df[f'{di}_{di_scale}_Class'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4284e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.sort_values(by=['Station_ID', 'Year', 'Month'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e02915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns_lag_roll = [\n",
    "    'ERA5_Precipitation',\n",
    "    'GPM_Precipitation',\n",
    "    'TRMM_Precipitation',\n",
    "    'TERRACLIMATE_Precipitation',\n",
    "    'PERSIANNCDR_Precipitation',\n",
    "    f'GPM_{di}_{di_scale}',\n",
    "    f'ERA5_{di}_{di_scale}',\n",
    "    'PET_MOD16A2GF',\n",
    "    'NDVI', 'EVI',\n",
    "    'LSTDay', 'LSTNight', 'LST',\n",
    "    'PCI_GPM', 'PCI_ERA5',\n",
    "    'VCI', 'TCI', 'VHI',\n",
    "    'CI_GPM', 'CI_ERA5',\n",
    "]\n",
    "\n",
    "# Add Lag\n",
    "for lag in range(1, 4):\n",
    "    for col in selected_columns_lag_roll:\n",
    "        df[f'{col}_lag_{lag}'] = df.groupby('Station_ID', observed=False)[col].shift(lag)\n",
    "\n",
    "\n",
    "# Add Mean and Std Roll\n",
    "for r in [3, 6]:\n",
    "    for col in selected_columns_lag_roll:\n",
    "        df[f'{col}_roll_mean_{r}'] = df.groupby('Station_ID', observed=False)[col].transform(lambda x: x.rolling(window=r, min_periods=1).mean())\n",
    "        df[f'{col}_roll_std_{r}'] = df.groupby('Station_ID', observed=False)[col].transform(lambda x: x.rolling(window=r, min_periods=1).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a11e19b",
   "metadata": {},
   "source": [
    "# Remove all NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "689557a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b02cb",
   "metadata": {},
   "source": [
    "# Station-wise Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00653016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No classes with fewer than 2 samples found.\n",
      "Target Classes: ['ED' 'EW' 'MD' 'MW' 'NN' 'SD' 'VW']\n",
      "Training set shape: (1627, 166)\n",
      "Testing set shape: (698, 166)\n"
     ]
    }
   ],
   "source": [
    "# Define columns to exclude from features\n",
    "EXCLUDE_COLS = ['Station_Name', 'Station_ID', 'Date', f'{di}_{di_scale}_Class', f'{di}_{di_scale}', 'Year', 'Month']\n",
    "FEATURES = [col for col in df.columns if col not in EXCLUDE_COLS]\n",
    "\n",
    "# Station-wise standardization\n",
    "df_scaled = df.copy()\n",
    "for station in df_scaled['Station_Name'].unique():\n",
    "    station_mask = df_scaled['Station_Name'] == station\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled.loc[station_mask, FEATURES] = scaler.fit_transform(df_scaled.loc[station_mask, FEATURES])\n",
    "\n",
    "\n",
    "class_counts = df_scaled[f'{di}_{di_scale}_Class'].value_counts()\n",
    "classes_to_remove = class_counts[class_counts < 2].index\n",
    "\n",
    "if not classes_to_remove.empty:\n",
    "    print(f\"Removing classes with fewer than 2 samples: {classes_to_remove.tolist()}\")\n",
    "    \n",
    "    # Filter the dataframe to exclude these rare classes\n",
    "    df_filtered = df_scaled[~df_scaled[f'{di}_{di_scale}_Class'].isin(classes_to_remove)].copy()\n",
    "    \n",
    "    # Redefine X and y from the filtered dataframe\n",
    "    X = df_filtered[FEATURES]\n",
    "    y = df_filtered[f'{di}_{di_scale}_Class']\n",
    "    \n",
    "    print(f\"Data shape after removing rare classes: {X.shape}\")\n",
    "    print(\"\\n--- Final Class Distribution ---\")\n",
    "    print(y.value_counts())\n",
    "else:\n",
    "    print(\"No classes with fewer than 2 samples found.\")\n",
    "    X = df_scaled[FEATURES]\n",
    "    y = df_scaled[f'{di}_{di_scale}_Class']\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y_encoded,\n",
    "    test_size=0.30,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Target Classes: {le.classes_}\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c0534b",
   "metadata": {},
   "source": [
    "# Feature Selection with RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "395cf562",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      7\u001b[39m rfecv = RFECV(\n\u001b[32m      8\u001b[39m     estimator=estimator,\n\u001b[32m      9\u001b[39m     step=\u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Fit RFECV on the training data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mrfecv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOptimal number of features found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrfecv.n_features_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Get the final selected features\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/w/DroughtMonitoringIran/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:63\u001b[39m, in \u001b[36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     61\u001b[39m extra_args = \u001b[38;5;28mlen\u001b[39m(args) - \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extra_args <= \u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[32m     66\u001b[39m args_msg = [\n\u001b[32m     67\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(name, arg)\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[-extra_args:])\n\u001b[32m     69\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/w/DroughtMonitoringIran/.venv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/w/DroughtMonitoringIran/.venv/lib/python3.12/site-packages/sklearn/feature_selection/_rfe.py:902\u001b[39m, in \u001b[36mRFECV.fit\u001b[39m\u001b[34m(self, X, y, groups, **params)\u001b[39m\n\u001b[32m    899\u001b[39m     parallel = Parallel(n_jobs=\u001b[38;5;28mself\u001b[39m.n_jobs)\n\u001b[32m    900\u001b[39m     func = delayed(_rfe_single_fit)\n\u001b[32m--> \u001b[39m\u001b[32m902\u001b[39m step_results = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrfe\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    906\u001b[39m scores, supports, rankings, step_n_features = \u001b[38;5;28mzip\u001b[39m(*step_results)\n\u001b[32m    908\u001b[39m step_n_features_rev = np.array(step_n_features[\u001b[32m0\u001b[39m])[::-\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/w/DroughtMonitoringIran/.venv/lib/python3.12/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/w/DroughtMonitoringIran/.venv/lib/python3.12/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/w/DroughtMonitoringIran/.venv/lib/python3.12/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/w/DroughtMonitoringIran/.venv/lib/python3.12/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# The estimator that will be used by RFECV\n",
    "estimator = GradientBoostingClassifier(\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# The RFECV object\n",
    "rfecv = RFECV(\n",
    "    estimator=estimator,\n",
    "    step=1,\n",
    "    cv=StratifiedKFold(5),\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit RFECV on the training data\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimal number of features found: {rfecv.n_features_}\")\n",
    "\n",
    "# Get the final selected features\n",
    "final_features = X_train.columns[rfecv.support_]\n",
    "print(f\"Selected features: {final_features.tolist()}\")\n",
    "\n",
    "# --- Plot Feature Importances ---\n",
    "# The `rfecv.estimator_` attribute is the model trained on the full set of features\n",
    "# during the last step of the cross-validation process. We can use its feature importances.\n",
    "importances = rfecv.estimator_.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': final_features,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot only the selected features\n",
    "selected_feature_importances = feature_importance_df[feature_importance_df['Feature'].isin(final_features)]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=selected_feature_importances)\n",
    "plt.title('Feature Importances for Selected Features (from RFECV)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Update training and testing sets with selected features\n",
    "X_train_final = X_train[final_features]\n",
    "X_test_final = X_test[final_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948efbdd",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b28a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Define the objective function for Optuna to optimize.\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'random_state': 42,\n",
    "    }\n",
    "\n",
    "    model = GradientBoostingClassifier(**params)\n",
    "    \n",
    "    # Stratified K-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X_train_final, y_train):\n",
    "        X_train_fold, X_val_fold = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        preds = model.predict(X_val_fold)\n",
    "        f1_scores.append(f1_score(y_val_fold, preds, average='macro'))\n",
    "        \n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "# Create a study object and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, timeout=600) # 50 trials or 10 minutes\n",
    "\n",
    "print(f\"Best trial found: {study.best_trial.number}\")\n",
    "print(f\"Best Macro-F1 Score: {study.best_value}\")\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48541f6a",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6cdd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_params['random_state'] = 42\n",
    "\n",
    "# Train the final model\n",
    "final_model = GradientBoostingClassifier(**best_params)\n",
    "final_model.fit(X_train_final, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = final_model.predict(X_test_final)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro-F1 Score: {macro_f1:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "\n",
    "# --- Plot Confusion Matrix ---\n",
    "print(\"\\n--- Plotting Confusion Matrix ---\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a48236",
   "metadata": {},
   "source": [
    "# Explainability with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf13db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer for the final model\n",
    "# Using a subset of the training data for the background is a common practice for performance\n",
    "explainer = shap.TreeExplainer(final_model, X_train_final.sample(100, random_state=42))\n",
    "shap_values = explainer.shap_values(X_test_final)\n",
    "\n",
    "# SHAP Summary Plot (Bar)\n",
    "print(\"\\nGenerating SHAP Summary Bar Plot...\")\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_test_final, plot_type=\"bar\", class_names=le.classes_, show=False)\n",
    "plt.title(\"SHAP Summary Plot (Global Feature Importance)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# SHAP Summary Plot (Beeswarm)\n",
    "print(\"\\nGenerating SHAP Summary Beeswarm Plot...\")\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_test_final, class_names=le.classes_, show=False,  plot_type=\"violin\")\n",
    "plt.title(\"SHAP Summary Plot (Beeswarm)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "droughtmonitoringiran (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
