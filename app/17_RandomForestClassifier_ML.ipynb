{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466fb799",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c141a822",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier # Changed from ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27845aff",
   "metadata": {},
   "source": [
    "# Load Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1361a6af",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('/home/pooya/w/DroughtMonitoringIran/')\n",
    "\n",
    "DATABASE_PATH = \"./database/database.db\"\n",
    "\n",
    "RESULTS_DIR = 'results'\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "conn = sqlite3.connect(DATABASE_PATH)\n",
    "\n",
    "data = pd.read_sql(sql='SELECT * FROM data', con=conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239c059a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No classes with fewer than 2 samples found.\n",
      "Target Classes: ['ED' 'EW' 'MD' 'MW' 'NN' 'SD' 'VW']\n",
      "Training set shape: (2121, 74)\n",
      "Testing set shape: (909, 74)\n",
      "Optimal number of features found: 40\n",
      "Selected features: ['GPM_SPI_1', 'ERA5_SPI_1', 'ERA5_Precipitation', 'GPM_Precipitation', 'PET_MOD16A2GF', 'NDVI', 'LSTDay', 'LST', 'PCI_GPM', 'PCI_ERA5', 'TCI', 'VHI', 'CI_GPM', 'CI_ERA5', 'LST_Diff', 'ERA5_Precipitation_lag_1', 'GPM_SPI_1_lag_1', 'PET_MOD16A2GF_lag_1', 'NDVI_lag_1', 'EVI_lag_1', 'PCI_GPM_lag_1', 'PCI_ERA5_lag_1', 'TCI_lag_1', 'VHI_lag_1', 'CI_GPM_lag_1', 'ERA5_Precipitation_lag_2', 'GPM_SPI_1_lag_2', 'ERA5_SPI_1_lag_2', 'EVI_lag_2', 'LSTDay_lag_2', 'LSTNight_lag_2', 'VCI_lag_2', 'TCI_lag_2', 'ERA5_Precipitation_lag_3', 'GPM_Precipitation_lag_3', 'EVI_lag_3', 'LSTNight_lag_3', 'PCI_GPM_lag_3', 'TCI_lag_3', 'CI_GPM_lag_3']\n",
      "Selected features saved to results/ALL/SPI_1/selected_features.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-24 02:03:15,209] A new study created in memory with name: no-name-ca7b3901-6a9d-4667-ab54-c18079dec7fa\n",
      "[I 2025-08-24 02:03:18,489] Trial 0 finished with value: 0.23832738050857388 and parameters: {'n_estimators': 204, 'max_depth': 48, 'min_samples_split': 14, 'min_samples_leaf': 15, 'max_features': 0.4004911152393068, 'criterion': 'gini'}. Best is trial 0 with value: 0.23832738050857388.\n",
      "[I 2025-08-24 02:03:26,344] Trial 1 finished with value: 0.2732131088224911 and parameters: {'n_estimators': 427, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 0.9327846806191983, 'criterion': 'gini'}. Best is trial 1 with value: 0.2732131088224911.\n",
      "[I 2025-08-24 02:03:33,392] Trial 2 finished with value: 0.26415662824950115 and parameters: {'n_estimators': 258, 'max_depth': 10, 'min_samples_split': 3, 'min_samples_leaf': 12, 'max_features': 0.46269354194336176, 'criterion': 'gini'}. Best is trial 1 with value: 0.2732131088224911.\n",
      "[I 2025-08-24 02:03:39,695] Trial 3 finished with value: 0.27601455651229145 and parameters: {'n_estimators': 464, 'max_depth': 24, 'min_samples_split': 12, 'min_samples_leaf': 3, 'max_features': 0.15831724262066602, 'criterion': 'entropy'}. Best is trial 3 with value: 0.27601455651229145.\n",
      "[I 2025-08-24 02:03:43,681] Trial 4 finished with value: 0.2944422776775796 and parameters: {'n_estimators': 220, 'max_depth': 25, 'min_samples_split': 11, 'min_samples_leaf': 5, 'max_features': 0.35421422333784447, 'criterion': 'entropy'}. Best is trial 4 with value: 0.2944422776775796.\n",
      "[I 2025-08-24 02:03:44,718] Trial 5 finished with value: 0.29878754050145834 and parameters: {'n_estimators': 55, 'max_depth': 10, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': 0.3676168801704508, 'criterion': 'gini'}. Best is trial 5 with value: 0.29878754050145834.\n",
      "[I 2025-08-24 02:03:48,812] Trial 6 finished with value: 0.21515783203819 and parameters: {'n_estimators': 284, 'max_depth': 24, 'min_samples_split': 3, 'min_samples_leaf': 17, 'max_features': 0.2604986484031724, 'criterion': 'entropy'}. Best is trial 5 with value: 0.29878754050145834.\n",
      "[I 2025-08-24 02:03:51,936] Trial 7 finished with value: 0.23089232214275324 and parameters: {'n_estimators': 227, 'max_depth': 14, 'min_samples_split': 9, 'min_samples_leaf': 16, 'max_features': 0.2813281660502701, 'criterion': 'gini'}. Best is trial 5 with value: 0.29878754050145834.\n",
      "[I 2025-08-24 02:03:55,066] Trial 8 finished with value: 0.24260674889965914 and parameters: {'n_estimators': 143, 'max_depth': 12, 'min_samples_split': 17, 'min_samples_leaf': 20, 'max_features': 0.6333161639094761, 'criterion': 'entropy'}. Best is trial 5 with value: 0.29878754050145834.\n",
      "[I 2025-08-24 02:03:59,572] Trial 9 finished with value: 0.27280933078799413 and parameters: {'n_estimators': 143, 'max_depth': 16, 'min_samples_split': 6, 'min_samples_leaf': 10, 'max_features': 0.8805952193475036, 'criterion': 'entropy'}. Best is trial 5 with value: 0.29878754050145834.\n",
      "[I 2025-08-24 02:04:00,912] Trial 10 finished with value: 0.2959708956696881 and parameters: {'n_estimators': 50, 'max_depth': 38, 'min_samples_split': 19, 'min_samples_leaf': 7, 'max_features': 0.6336260514610267, 'criterion': 'gini'}. Best is trial 5 with value: 0.29878754050145834.\n",
      "[I 2025-08-24 02:04:02,693] Trial 11 finished with value: 0.2902599142911033 and parameters: {'n_estimators': 71, 'max_depth': 39, 'min_samples_split': 20, 'min_samples_leaf': 7, 'max_features': 0.6468598960183133, 'criterion': 'gini'}. Best is trial 5 with value: 0.29878754050145834.\n",
      "[I 2025-08-24 02:04:04,309] Trial 12 finished with value: 0.3090172120796389 and parameters: {'n_estimators': 54, 'max_depth': 36, 'min_samples_split': 20, 'min_samples_leaf': 8, 'max_features': 0.7870069173335879, 'criterion': 'gini'}. Best is trial 12 with value: 0.3090172120796389.\n",
      "[I 2025-08-24 02:04:10,268] Trial 13 finished with value: 0.2955484754343306 and parameters: {'n_estimators': 118, 'max_depth': 33, 'min_samples_split': 16, 'min_samples_leaf': 10, 'max_features': 0.7638249139970044, 'criterion': 'gini'}. Best is trial 12 with value: 0.3090172120796389.\n",
      "[I 2025-08-24 02:04:19,619] Trial 14 finished with value: 0.30822567830387804 and parameters: {'n_estimators': 332, 'max_depth': 45, 'min_samples_split': 15, 'min_samples_leaf': 6, 'max_features': 0.8033334252017716, 'criterion': 'gini'}. Best is trial 12 with value: 0.3090172120796389.\n",
      "[I 2025-08-24 02:04:28,882] Trial 15 finished with value: 0.3038969888756248 and parameters: {'n_estimators': 346, 'max_depth': 50, 'min_samples_split': 17, 'min_samples_leaf': 8, 'max_features': 0.8066549774095197, 'criterion': 'gini'}. Best is trial 12 with value: 0.3090172120796389.\n",
      "[I 2025-08-24 02:04:38,009] Trial 16 finished with value: 0.27950850431756635 and parameters: {'n_estimators': 351, 'max_depth': 43, 'min_samples_split': 15, 'min_samples_leaf': 13, 'max_features': 0.9813930799607684, 'criterion': 'gini'}. Best is trial 12 with value: 0.3090172120796389.\n",
      "[I 2025-08-24 02:04:50,479] Trial 17 finished with value: 0.3110617816554904 and parameters: {'n_estimators': 345, 'max_depth': 32, 'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 0.7545074604085072, 'criterion': 'gini'}. Best is trial 17 with value: 0.3110617816554904.\n",
      "[I 2025-08-24 02:05:05,949] Trial 18 finished with value: 0.31017508411118555 and parameters: {'n_estimators': 419, 'max_depth': 31, 'min_samples_split': 19, 'min_samples_leaf': 1, 'max_features': 0.5380157793393135, 'criterion': 'gini'}. Best is trial 17 with value: 0.3110617816554904.\n",
      "[I 2025-08-24 02:05:16,580] Trial 19 finished with value: 0.31070692907920583 and parameters: {'n_estimators': 402, 'max_depth': 32, 'min_samples_split': 19, 'min_samples_leaf': 1, 'max_features': 0.5250148018119843, 'criterion': 'gini'}. Best is trial 17 with value: 0.3110617816554904.\n",
      "[I 2025-08-24 02:05:24,674] Trial 20 finished with value: 0.3074109401675318 and parameters: {'n_estimators': 379, 'max_depth': 20, 'min_samples_split': 18, 'min_samples_leaf': 3, 'max_features': 0.5331348383323945, 'criterion': 'gini'}. Best is trial 17 with value: 0.3110617816554904.\n",
      "[I 2025-08-24 02:05:39,627] Trial 21 finished with value: 0.30552123266219844 and parameters: {'n_estimators': 500, 'max_depth': 31, 'min_samples_split': 20, 'min_samples_leaf': 1, 'max_features': 0.49542575205627054, 'criterion': 'gini'}. Best is trial 17 with value: 0.3110617816554904.\n",
      "[I 2025-08-24 02:05:51,362] Trial 22 finished with value: 0.3057210729303458 and parameters: {'n_estimators': 409, 'max_depth': 30, 'min_samples_split': 18, 'min_samples_leaf': 3, 'max_features': 0.692356408548634, 'criterion': 'gini'}. Best is trial 17 with value: 0.3110617816554904.\n",
      "[I 2025-08-24 02:06:03,100] Trial 23 finished with value: 0.32340214247747345 and parameters: {'n_estimators': 423, 'max_depth': 35, 'min_samples_split': 18, 'min_samples_leaf': 1, 'max_features': 0.5644163901575412, 'criterion': 'gini'}. Best is trial 23 with value: 0.32340214247747345.\n",
      "[I 2025-08-24 02:06:15,646] Trial 24 finished with value: 0.32630806463138773 and parameters: {'n_estimators': 308, 'max_depth': 35, 'min_samples_split': 14, 'min_samples_leaf': 3, 'max_features': 0.7188288190902736, 'criterion': 'gini'}. Best is trial 24 with value: 0.32630806463138773.\n",
      "[I 2025-08-24 02:06:24,707] Trial 25 finished with value: 0.3012889822734445 and parameters: {'n_estimators': 318, 'max_depth': 42, 'min_samples_split': 14, 'min_samples_leaf': 4, 'max_features': 0.7060123227601498, 'criterion': 'gini'}. Best is trial 24 with value: 0.32630806463138773.\n",
      "[I 2025-08-24 02:06:31,773] Trial 26 finished with value: 0.29424568146165686 and parameters: {'n_estimators': 287, 'max_depth': 35, 'min_samples_split': 16, 'min_samples_leaf': 3, 'max_features': 0.5932048787594137, 'criterion': 'entropy'}. Best is trial 24 with value: 0.32630806463138773.\n",
      "[I 2025-08-24 02:06:45,820] Trial 27 finished with value: 0.3220491445190829 and parameters: {'n_estimators': 377, 'max_depth': 27, 'min_samples_split': 14, 'min_samples_leaf': 2, 'max_features': 0.866716441719674, 'criterion': 'gini'}. Best is trial 24 with value: 0.32630806463138773.\n",
      "[I 2025-08-24 02:07:00,311] Trial 28 finished with value: 0.31417086078960776 and parameters: {'n_estimators': 373, 'max_depth': 28, 'min_samples_split': 13, 'min_samples_leaf': 5, 'max_features': 0.8716517431702077, 'criterion': 'gini'}. Best is trial 24 with value: 0.32630806463138773.\n",
      "[I 2025-08-24 02:07:15,656] Trial 29 finished with value: 0.32345293725594443 and parameters: {'n_estimators': 459, 'max_depth': 19, 'min_samples_split': 10, 'min_samples_leaf': 3, 'max_features': 0.8912474087579634, 'criterion': 'gini'}. Best is trial 24 with value: 0.32630806463138773.\n",
      "[I 2025-08-24 02:07:27,299] Trial 30 finished with value: 0.3202659925305265 and parameters: {'n_estimators': 461, 'max_depth': 21, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 0.4360976106063284, 'criterion': 'gini'}. Best is trial 24 with value: 0.32630806463138773.\n",
      "[I 2025-08-24 02:07:41,912] Trial 31 finished with value: 0.32612860586048553 and parameters: {'n_estimators': 446, 'max_depth': 18, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 0.8981303679046864, 'criterion': 'gini'}. Best is trial 24 with value: 0.32630806463138773.\n",
      "[I 2025-08-24 02:07:57,634] Trial 32 finished with value: 0.32758578343297395 and parameters: {'n_estimators': 457, 'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 0.9382813249566467, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:08:15,697] Trial 33 finished with value: 0.3218935180936216 and parameters: {'n_estimators': 464, 'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 4, 'max_features': 0.9673846027633723, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:08:25,913] Trial 34 finished with value: 0.3123154197254946 and parameters: {'n_estimators': 497, 'max_depth': 7, 'min_samples_split': 8, 'min_samples_leaf': 2, 'max_features': 0.9176574297229799, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:08:40,194] Trial 35 finished with value: 0.3172421681591181 and parameters: {'n_estimators': 445, 'max_depth': 21, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 0.9172135052654947, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:08:59,562] Trial 36 finished with value: 0.30729788315916 and parameters: {'n_estimators': 446, 'max_depth': 16, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 0.8421501794917378, 'criterion': 'entropy'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:09:10,544] Trial 37 finished with value: 0.30338583851696926 and parameters: {'n_estimators': 489, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 6, 'max_features': 0.9988108877380152, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:09:16,036] Trial 38 finished with value: 0.26745726217990856 and parameters: {'n_estimators': 254, 'max_depth': 25, 'min_samples_split': 8, 'min_samples_leaf': 12, 'max_features': 0.7205416715292065, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:09:36,578] Trial 39 finished with value: 0.3063148849844241 and parameters: {'n_estimators': 479, 'max_depth': 12, 'min_samples_split': 11, 'min_samples_leaf': 3, 'max_features': 0.9485408200021751, 'criterion': 'entropy'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:09:41,788] Trial 40 finished with value: 0.2885384516018722 and parameters: {'n_estimators': 186, 'max_depth': 23, 'min_samples_split': 2, 'min_samples_leaf': 9, 'max_features': 0.8334024687666367, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:09:56,655] Trial 41 finished with value: 0.3201623244593517 and parameters: {'n_estimators': 438, 'max_depth': 18, 'min_samples_split': 12, 'min_samples_leaf': 2, 'max_features': 0.9077758507590497, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:10:07,352] Trial 42 finished with value: 0.30525496445236266 and parameters: {'n_estimators': 390, 'max_depth': 15, 'min_samples_split': 8, 'min_samples_leaf': 4, 'max_features': 0.6663285919289605, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:10:18,716] Trial 43 finished with value: 0.3258362001306683 and parameters: {'n_estimators': 425, 'max_depth': 28, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 0.5785027287628235, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:10:28,581] Trial 44 finished with value: 0.3070444259110353 and parameters: {'n_estimators': 472, 'max_depth': 27, 'min_samples_split': 10, 'min_samples_leaf': 6, 'max_features': 0.5980992021088443, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:10:38,495] Trial 45 finished with value: 0.24162920351764758 and parameters: {'n_estimators': 445, 'max_depth': 18, 'min_samples_split': 9, 'min_samples_leaf': 18, 'max_features': 0.7420987844020417, 'criterion': 'entropy'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:10:47,833] Trial 46 finished with value: 0.3231039868264314 and parameters: {'n_estimators': 406, 'max_depth': 13, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 0.30951669560452694, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:11:01,546] Trial 47 finished with value: 0.32079226264042926 and parameters: {'n_estimators': 422, 'max_depth': 23, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 0.8872074226090911, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:11:07,014] Trial 48 finished with value: 0.2455016618152913 and parameters: {'n_estimators': 456, 'max_depth': 10, 'min_samples_split': 12, 'min_samples_leaf': 5, 'max_features': 0.1066090334458698, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n",
      "[I 2025-08-24 02:11:20,656] Trial 49 finished with value: 0.315831217935667 and parameters: {'n_estimators': 316, 'max_depth': 38, 'min_samples_split': 11, 'min_samples_leaf': 3, 'max_features': 0.8284529599362198, 'criterion': 'gini'}. Best is trial 32 with value: 0.32758578343297395.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters saved to results/ALL/SPI_1/best_params.json\n",
      "Best trial found: 32\n",
      "Best Macro-F1 Score: 0.32758578343297395\n",
      "Best Hyperparameters:\n",
      "{'n_estimators': 457, 'max_depth': 18, 'min_samples_split': 7, 'min_samples_leaf': 2, 'max_features': 0.9382813249566467, 'criterion': 'gini'}\n",
      "Evaluation metrics saved to results/ALL/SPI_1/evaluation_metrics.txt\n",
      "Overall Accuracy: 0.6755\n",
      "Macro-F1 Score: 0.2439\n",
      "Cohen's Kappa: 0.2299\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ED       0.29      0.10      0.15        41\n",
      "          EW       0.25      0.14      0.18        21\n",
      "          MD       0.11      0.04      0.05        55\n",
      "          MW       0.33      0.19      0.24        97\n",
      "          NN       0.75      0.95      0.83       608\n",
      "          SD       0.00      0.00      0.00        27\n",
      "          VW       0.34      0.20      0.25        60\n",
      "\n",
      "    accuracy                           0.68       909\n",
      "   macro avg       0.30      0.23      0.24       909\n",
      "weighted avg       0.58      0.68      0.61       909\n",
      "\n",
      "\n",
      "--- Plotting Confusion Matrix ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|=================   | 5257/6363 [04:14<00:53]       "
     ]
    }
   ],
   "source": [
    "def run(di, di_scale, selected_stations, stations_group_name, start_date, end_date):\n",
    "    \n",
    "    group_dir = os.path.join(RESULTS_DIR, stations_group_name)\n",
    "    os.makedirs(group_dir, exist_ok=True)\n",
    "    \n",
    "    scale_dir = os.path.join(group_dir, f'{di}_{di_scale}')\n",
    "    os.makedirs(scale_dir, exist_ok=True)\n",
    "    \n",
    "    # Select Columns\n",
    "    selected_columns = [\n",
    "        'Station_Name', 'Station_ID',\n",
    "        'Station_Latitude', 'Station_Longitude', 'Station_Elevation',\n",
    "        'Date',\n",
    "        f'{di}_{di_scale}',\n",
    "        f'GPM_{di}_{di_scale}',\n",
    "        f'ERA5_{di}_{di_scale}',\n",
    "        'ERA5_Precipitation',\n",
    "        'GPM_Precipitation',\n",
    "        'PET_MOD16A2GF',\n",
    "        'NDVI', 'EVI',\n",
    "        'LSTDay', 'LSTNight', 'LST',\n",
    "        'PCI_GPM', 'PCI_ERA5',\n",
    "        'VCI', 'TCI', 'VHI',\n",
    "        'CI_GPM', 'CI_ERA5',\n",
    "    ]\n",
    "    \n",
    "    df = data\\\n",
    "    .filter(items=selected_columns)\\\n",
    "        .query(\"Station_Name in @selected_stations and Date >= @start_date and Date < @end_date\")\n",
    "\n",
    "    # Date, Year, Month\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m')\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "\n",
    "    # SPI and SPEI Class\n",
    "    df[f'{di}_{di_scale}_Class'] = pd.cut(df[f'{di}_{di_scale}'], bins=[-10, -2, -1.5, -1, 1, 1.5, 2, 10], labels=['ED', 'SD', 'MD', 'NN', 'MW', 'VW', 'EW'])\n",
    "\n",
    "    # Month Sin & Cos\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "            \n",
    "    # LST Diff\n",
    "    df['LST_Diff'] = df['LSTDay'] - df['LSTNight']\n",
    "\n",
    "    # Convert to Category\n",
    "    df['Station_ID'] = df['Station_ID'].astype('category')\n",
    "    df['Year'] = df['Year'].astype('category')\n",
    "    df['Month'] = df['Month'].astype('category')\n",
    "    df[f'{di}_{di_scale}_Class'] = df[f'{di}_{di_scale}_Class'].astype('category')\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    df.sort_values(by=['Station_ID', 'Year', 'Month'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    selected_columns_lag_roll = [\n",
    "        'ERA5_Precipitation',\n",
    "        'GPM_Precipitation',\n",
    "        f'GPM_{di}_{di_scale}',\n",
    "        f'ERA5_{di}_{di_scale}',\n",
    "        'PET_MOD16A2GF',\n",
    "        'NDVI', 'EVI',\n",
    "        'LSTDay', 'LSTNight', 'LST',\n",
    "        'PCI_GPM', 'PCI_ERA5',\n",
    "        'VCI', 'TCI', 'VHI',\n",
    "        'CI_GPM', 'CI_ERA5',\n",
    "    ]\n",
    "\n",
    "    # Add Lag\n",
    "    for lag in range(1, 4):\n",
    "        for col in selected_columns_lag_roll:\n",
    "            df[f'{col}_lag_{lag}'] = df.groupby('Station_ID', observed=False)[col].shift(lag)\n",
    "\n",
    "\n",
    "    # # Add Mean and Std Roll\n",
    "    # for r in [3, 6]:\n",
    "    #     for col in selected_columns_lag_roll:\n",
    "    #         df[f'{col}_roll_mean_{r}'] = df.groupby('Station_ID', observed=False)[col].transform(lambda x: x.rolling(window=r, min_periods=1).mean())\n",
    "    #         df[f'{col}_roll_std_{r}'] = df.groupby('Station_ID', observed=False)[col].transform(lambda x: x.rolling(window=r, min_periods=1).std())\n",
    "    \n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Define columns to exclude from features\n",
    "    EXCLUDE_COLS = ['Station_Name', 'Station_ID', 'Date', f'{di}_{di_scale}_Class', f'{di}_{di_scale}', 'Year', 'Month']\n",
    "    FEATURES = [col for col in df.columns if col not in EXCLUDE_COLS]\n",
    "\n",
    "    # Station-wise standardization\n",
    "    df_scaled = df.copy()\n",
    "    for station in df_scaled['Station_Name'].unique():\n",
    "        station_mask = df_scaled['Station_Name'] == station\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled.loc[station_mask, FEATURES] = scaler.fit_transform(df_scaled.loc[station_mask, FEATURES])\n",
    "\n",
    "\n",
    "    class_counts = df_scaled[f'{di}_{di_scale}_Class'].value_counts()\n",
    "    classes_to_remove = class_counts[class_counts < 2].index\n",
    "\n",
    "    if not classes_to_remove.empty:\n",
    "        print(f\"Removing classes with fewer than 2 samples: {classes_to_remove.tolist()}\")\n",
    "        \n",
    "        # Filter the dataframe to exclude these rare classes\n",
    "        df_filtered = df_scaled[~df_scaled[f'{di}_{di_scale}_Class'].isin(classes_to_remove)].copy()\n",
    "        \n",
    "        # Redefine X and y from the filtered dataframe\n",
    "        X = df_filtered[FEATURES]\n",
    "        y = df_filtered[f'{di}_{di_scale}_Class']\n",
    "        \n",
    "        print(f\"Data shape after removing rare classes: {X.shape}\")\n",
    "        print(\"\\n--- Final Class Distribution ---\")\n",
    "        print(y.value_counts())\n",
    "    else:\n",
    "        print(\"No classes with fewer than 2 samples found.\")\n",
    "        X = df_scaled[FEATURES]\n",
    "        y = df_scaled[f'{di}_{di_scale}_Class']\n",
    "\n",
    "    # Encode the target variable\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y_encoded,\n",
    "        test_size=0.30,\n",
    "        random_state=42,\n",
    "        stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    print(f\"Target Classes: {le.classes_}\")\n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "    # The estimator that will be used by RFECV\n",
    "    estimator = RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # The RFECV object\n",
    "    rfecv = RFECV(\n",
    "        estimator=estimator,\n",
    "        step=1,\n",
    "        cv=StratifiedKFold(5),\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Fit RFECV on the training data\n",
    "    rfecv.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Optimal number of features found: {rfecv.n_features_}\")\n",
    "\n",
    "    # Get the final selected features\n",
    "    final_features = X_train.columns[rfecv.support_]\n",
    "    print(f\"Selected features: {final_features.tolist()}\")\n",
    "    \n",
    "    features_path = os.path.join(scale_dir, 'selected_features.txt')\n",
    "    with open(features_path, 'w') as f:\n",
    "        for feature in final_features:\n",
    "            f.write(f\"{feature}\\n\")\n",
    "    print(f\"Selected features saved to {features_path}\")\n",
    "\n",
    "    # --- Plot Feature Importances ---\n",
    "    # The `rfecv.estimator_` attribute is the model trained on the full set of features\n",
    "    # during the last step of the cross-validation process. We can use its feature importances.\n",
    "    importances = rfecv.estimator_.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': final_features,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot only the selected features\n",
    "    selected_feature_importances = feature_importance_df[feature_importance_df['Feature'].isin(final_features)]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=selected_feature_importances)\n",
    "    plt.title('Feature Importances for Selected Features (from RFECV)')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(scale_dir, 'feature_importances.png')\n",
    "    plt.savefig(fig_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Update training and testing sets with selected features\n",
    "    X_train_final = X_train[final_features]\n",
    "    X_test_final = X_test[final_features]\n",
    "    \n",
    "    \n",
    "    def objective(trial):\n",
    "        \"\"\"Define the objective function for Optuna to optimize.\"\"\"\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "            'max_features': trial.suggest_float('max_features', 0.1, 1.0),\n",
    "            'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        model = RandomForestClassifier(**params)\n",
    "        \n",
    "        # Stratified K-Fold Cross-Validation\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        f1_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in skf.split(X_train_final, y_train):\n",
    "            X_train_fold, X_val_fold = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "            \n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            preds = model.predict(X_val_fold)\n",
    "            f1_scores.append(f1_score(y_val_fold, preds, average='macro'))\n",
    "            \n",
    "        return np.mean(f1_scores)\n",
    "\n",
    "    # Create a study object and optimize\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=50, timeout=600) # 50 trials or 10 minutes\n",
    "\n",
    "    # Save best parameters\n",
    "    best_params = study.best_params \n",
    "    params_path = os.path.join(scale_dir, 'best_params.json')\n",
    "    with open(params_path, 'w') as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "    print(f\"Best parameters saved to {params_path}\")\n",
    "\n",
    "    print(f\"Best trial found: {study.best_trial.number}\")\n",
    "    print(f\"Best Macro-F1 Score: {study.best_value}\")\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "\n",
    "\n",
    "    # Get best hyperparameters\n",
    "    best_params = study.best_params\n",
    "    best_params['random_state'] = 42\n",
    "    best_params['n_jobs'] = -1\n",
    "\n",
    "    # Train the final model\n",
    "    final_model = RandomForestClassifier(**best_params)\n",
    "    final_model.fit(X_train_final, y_train)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = final_model.predict(X_test_final)\n",
    "\n",
    "    # Save evaluation metrics\n",
    "    report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    \n",
    "    metrics_path = os.path.join(scale_dir, 'evaluation_metrics.txt')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        f.write(f\"Overall Accuracy: {accuracy:.4f}\\n\")\n",
    "        f.write(f\"Macro-F1 Score: {macro_f1:.4f}\\n\")\n",
    "        f.write(f\"Cohen's Kappa: {kappa:.4f}\\n\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(report)\n",
    "    print(f\"Evaluation metrics saved to {metrics_path}\")\n",
    "\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro-F1 Score: {macro_f1:.4f}\")\n",
    "    print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "\n",
    "    # --- Plot Confusion Matrix ---\n",
    "    print(\"\\n--- Plotting Confusion Matrix ---\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    fig_path = os.path.join(scale_dir, 'confusion_matrix.png')\n",
    "    plt.savefig(fig_path)\n",
    "    plt.close()\n",
    "        \n",
    "\n",
    "    # Create SHAP explainer for the final model\n",
    "    # Using a subset of the training data for the background is a common practice for performance\n",
    "    explainer = shap.TreeExplainer(final_model, X_train_final.sample(100, random_state=42))\n",
    "    shap_values = explainer.shap_values(X_test_final)\n",
    "\n",
    "    try:\n",
    "        # SHAP Summary Plot (Bar)\n",
    "        print(\"\\nGenerating SHAP Summary Bar Plot...\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, X_test_final, plot_type=\"bar\", class_names=le.classes_, show=False)\n",
    "        plt.title(\"SHAP Summary Plot (Global Feature Importance)\")\n",
    "        plt.tight_layout()\n",
    "        fig_path = os.path.join(scale_dir, 'shap_summary_bar.png')\n",
    "        plt.savefig(fig_path, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    except:\n",
    "        print(\"An exception occurred: Generating SHAP Summary Bar Plot...\")\n",
    "        \n",
    "    # try:\n",
    "    #     # SHAP Summary Plot (Beeswarm)\n",
    "    #     print(\"\\nGenerating SHAP Summary Beeswarm Plot...\")\n",
    "    #     plt.figure()\n",
    "    #     shap.summary_plot(shap_values, X_test_final, class_names=le.classes_, show=False,  plot_type=\"violin\")\n",
    "    #     plt.title(\"SHAP Summary Plot (Beeswarm)\")\n",
    "    #     plt.tight_layout()\n",
    "    #     fig_path = os.path.join(scale_dir, 'shap_summary_beeswarm.png')\n",
    "    #     plt.savefig(fig_path, bbox_inches='tight')\n",
    "    #     plt.close()\n",
    "    # except:\n",
    "    #     print(\"An exception occurred: SHAP Summary Plot (Beeswarm)\")\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "for di in ['SPI', 'SPEI']:\n",
    "    for di_scale in [1, 3, 6, 9, 12, 18, 24]:\n",
    "        for stations_group_name in ['ALL', 'C1', 'C2', 'C3']: \n",
    "\n",
    "            if stations_group_name == 'ALL':\n",
    "                selected_stations = ['Ramsar', 'Nowshahr', 'Siahbisheh', 'Kiyasar', 'Kojur', 'Baladeh', 'Alasht', 'Babolsar', 'Gharakhil', 'Sari', 'Sari (dasht-e-naz airport)', 'Galugah', 'Bandar-e-amirabad', 'Amol', 'Polsefid']\n",
    "            elif stations_group_name == 'C3':\n",
    "                selected_stations = ['Ramsar', 'Nowshahr']\n",
    "            elif stations_group_name == 'C2':\n",
    "                selected_stations = ['Siahbisheh', 'Kiyasar', 'Kojur', 'Baladeh', 'Alasht']\n",
    "            elif stations_group_name == 'C1':\n",
    "                selected_stations = ['Babolsar', 'Gharakhil', 'Sari', 'Sari (dasht-e-naz airport)', 'Galugah', 'Bandar-e-amirabad', 'Amol', 'Polsefid']\n",
    "            else:\n",
    "                selected_stations = ['Ramsar', 'Nowshahr', 'Siahbisheh', 'Kiyasar', 'Kojur', 'Baladeh', 'Alasht', 'Babolsar', 'Gharakhil', 'Sari', 'Sari (dasht-e-naz airport)', 'Galugah', 'Bandar-e-amirabad', 'Amol', 'Polsefid']\n",
    "\n",
    "            ex_dir = os.path.join(RESULTS_DIR, stations_group_name, f'{di}_{di_scale}')\n",
    "            if os.path.exists(ex_dir):\n",
    "                continue \n",
    "            \n",
    "            run(\n",
    "                di=di,\n",
    "                di_scale=di_scale,\n",
    "                selected_stations=selected_stations,\n",
    "                stations_group_name=stations_group_name,\n",
    "                start_date='2006-09', \n",
    "                end_date='2023-10'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94899866",
   "metadata": {},
   "source": [
    "# Select Columns and Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0de84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Drought Index and Period\n",
    "di = 'SPEI'\n",
    "di_scale = 3\n",
    "\n",
    "# Select Stations\n",
    "selected_stations = ['Ramsar', 'Nowshahr', 'Siahbisheh', 'Kiyasar', 'Kojur', 'Baladeh', 'Alasht', 'Babolsar', 'Gharakhil', 'Sari', 'Sari (dasht-e-naz airport)', 'Galugah', 'Bandar-e-amirabad', 'Amol', 'Polsefid']\n",
    "# selected_stations = ['Ramsar', 'Nowshahr']\n",
    "# selected_stations = ['Siahbisheh', 'Kiyasar', 'Kojur', 'Baladeh', 'Alasht']\n",
    "# selected_stations = ['Babolsar', 'Gharakhil', 'Sari', 'Sari (dasht-e-naz airport)', 'Galugah', 'Bandar-e-amirabad', 'Amol', 'Polsefid']\n",
    "\n",
    "# Select Columns\n",
    "selected_columns = [\n",
    "    'Station_Name', 'Station_ID',\n",
    "    'Station_Latitude', 'Station_Longitude', 'Station_Elevation',\n",
    "    'Date',\n",
    "    f'{di}_{di_scale}',\n",
    "    f'GPM_{di}_{di_scale}',\n",
    "    f'ERA5_{di}_{di_scale}',\n",
    "    'ERA5_Precipitation',\n",
    "    'GPM_Precipitation',\n",
    "    'PET_MOD16A2GF',\n",
    "    'NDVI', 'EVI',\n",
    "    'LSTDay', 'LSTNight', 'LST',\n",
    "    'PCI_GPM', 'PCI_ERA5',\n",
    "    'VCI', 'TCI', 'VHI',\n",
    "    'CI_GPM', 'CI_ERA5',\n",
    "]\n",
    "\n",
    "# Select Start and End Date\n",
    "start_date = '2006-09'\n",
    "end_date = '2023-10'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c773c79b",
   "metadata": {},
   "source": [
    "# Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf78173",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data\\\n",
    "    .filter(items=selected_columns)\\\n",
    "        .query(\"Station_Name in @selected_stations and Date >= @start_date and Date < @end_date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b1f34",
   "metadata": {},
   "source": [
    "# Add Some Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3303aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date, Year, Month\n",
    "df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m')\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "\n",
    "# SPI and SPEI Class\n",
    "df[f'{di}_{di_scale}_Class'] = pd.cut(df[f'{di}_{di_scale}'], bins=[-10, -2, -1.5, -1, 1, 1.5, 2, 10], labels=['ED', 'SD', 'MD', 'NN', 'MW', 'VW', 'EW'])\n",
    "\n",
    "# Month Sin & Cos\n",
    "df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "        \n",
    "# LST Diff\n",
    "df['LST_Diff'] = df['LSTDay'] - df['LSTNight']\n",
    "\n",
    "# Convert to Category\n",
    "df['Station_ID'] = df['Station_ID'].astype('category')\n",
    "df['Year'] = df['Year'].astype('category')\n",
    "df['Month'] = df['Month'].astype('category')\n",
    "df[f'{di}_{di_scale}_Class'] = df[f'{di}_{di_scale}_Class'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4284e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.sort_values(by=['Station_ID', 'Year', 'Month'], inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e02915e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns_lag_roll = [\n",
    "    'ERA5_Precipitation',\n",
    "    'GPM_Precipitation',\n",
    "    f'GPM_{di}_{di_scale}',\n",
    "    f'ERA5_{di}_{di_scale}',\n",
    "    'PET_MOD16A2GF',\n",
    "    'NDVI', 'EVI',\n",
    "    'LSTDay', 'LSTNight', 'LST',\n",
    "    'PCI_GPM', 'PCI_ERA5',\n",
    "    'VCI', 'TCI', 'VHI',\n",
    "    'CI_GPM', 'CI_ERA5',\n",
    "]\n",
    "\n",
    "# Add Lag\n",
    "for lag in range(1, 4):\n",
    "    for col in selected_columns_lag_roll:\n",
    "        df[f'{col}_lag_{lag}'] = df.groupby('Station_ID', observed=False)[col].shift(lag)\n",
    "\n",
    "\n",
    "# Add Mean and Std Roll\n",
    "for r in [3, 6]:\n",
    "    for col in selected_columns_lag_roll:\n",
    "        df[f'{col}_roll_mean_{r}'] = df.groupby('Station_ID', observed=False)[col].transform(lambda x: x.rolling(window=r, min_periods=1).mean())\n",
    "        df[f'{col}_roll_std_{r}'] = df.groupby('Station_ID', observed=False)[col].transform(lambda x: x.rolling(window=r, min_periods=1).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a11e19b",
   "metadata": {},
   "source": [
    "# Remove all NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689557a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059b02cb",
   "metadata": {},
   "source": [
    "# Station-wise Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00653016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns to exclude from features\n",
    "EXCLUDE_COLS = ['Station_Name', 'Station_ID', 'Date', f'{di}_{di_scale}_Class', f'{di}_{di_scale}', 'Year', 'Month']\n",
    "FEATURES = [col for col in df.columns if col not in EXCLUDE_COLS]\n",
    "\n",
    "# Station-wise standardization\n",
    "df_scaled = df.copy()\n",
    "for station in df_scaled['Station_Name'].unique():\n",
    "    station_mask = df_scaled['Station_Name'] == station\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled.loc[station_mask, FEATURES] = scaler.fit_transform(df_scaled.loc[station_mask, FEATURES])\n",
    "\n",
    "\n",
    "class_counts = df_scaled[f'{di}_{di_scale}_Class'].value_counts()\n",
    "classes_to_remove = class_counts[class_counts < 2].index\n",
    "\n",
    "if not classes_to_remove.empty:\n",
    "    print(f\"Removing classes with fewer than 2 samples: {classes_to_remove.tolist()}\")\n",
    "    \n",
    "    # Filter the dataframe to exclude these rare classes\n",
    "    df_filtered = df_scaled[~df_scaled[f'{di}_{di_scale}_Class'].isin(classes_to_remove)].copy()\n",
    "    \n",
    "    # Redefine X and y from the filtered dataframe\n",
    "    X = df_filtered[FEATURES]\n",
    "    y = df_filtered[f'{di}_{di_scale}_Class']\n",
    "    \n",
    "    print(f\"Data shape after removing rare classes: {X.shape}\")\n",
    "    print(\"\\n--- Final Class Distribution ---\")\n",
    "    print(y.value_counts())\n",
    "else:\n",
    "    print(\"No classes with fewer than 2 samples found.\")\n",
    "    X = df_scaled[FEATURES]\n",
    "    y = df_scaled[f'{di}_{di_scale}_Class']\n",
    "\n",
    "# Encode the target variable\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y_encoded,\n",
    "    test_size=0.30,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Target Classes: {le.classes_}\")\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Testing set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c0534b",
   "metadata": {},
   "source": [
    "# Feature Selection with RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395cf562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The estimator that will be used by RFECV\n",
    "estimator = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# The RFECV object\n",
    "rfecv = RFECV(\n",
    "    estimator=estimator,\n",
    "    step=1,\n",
    "    cv=StratifiedKFold(5),\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit RFECV on the training data\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Optimal number of features found: {rfecv.n_features_}\")\n",
    "\n",
    "# Get the final selected features\n",
    "final_features = X_train.columns[rfecv.support_]\n",
    "print(f\"Selected features: {final_features.tolist()}\")\n",
    "\n",
    "# --- Plot Feature Importances ---\n",
    "# The `rfecv.estimator_` attribute is the model trained on the full set of features\n",
    "# during the last step of the cross-validation process. We can use its feature importances.\n",
    "importances = rfecv.estimator_.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': final_features,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot only the selected features\n",
    "selected_feature_importances = feature_importance_df[feature_importance_df['Feature'].isin(final_features)]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=selected_feature_importances)\n",
    "plt.title('Feature Importances for Selected Features (from RFECV)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Update training and testing sets with selected features\n",
    "X_train_final = X_train[final_features]\n",
    "X_test_final = X_test[final_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948efbdd",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b28a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"Define the objective function for Optuna to optimize.\"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "        'max_features': trial.suggest_float('max_features', 0.1, 1.0),\n",
    "        'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    model = RandomForestClassifier(**params)\n",
    "    \n",
    "    # Stratified K-Fold Cross-Validation\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X_train_final, y_train):\n",
    "        X_train_fold, X_val_fold = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "        y_train_fold, y_val_fold = y_train[train_idx], y_train[val_idx]\n",
    "        \n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        preds = model.predict(X_val_fold)\n",
    "        f1_scores.append(f1_score(y_val_fold, preds, average='macro'))\n",
    "        \n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "# Create a study object and optimize\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, timeout=600) # 50 trials or 10 minutes\n",
    "\n",
    "print(f\"Best trial found: {study.best_trial.number}\")\n",
    "print(f\"Best Macro-F1 Score: {study.best_value}\")\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(study.best_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48541f6a",
   "metadata": {},
   "source": [
    "# Model Training & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6cdd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best hyperparameters\n",
    "best_params = study.best_params\n",
    "best_params['random_state'] = 42\n",
    "best_params['n_jobs'] = -1\n",
    "\n",
    "# Train the final model\n",
    "final_model = RandomForestClassifier(**best_params)\n",
    "final_model.fit(X_train_final, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = final_model.predict(X_test_final)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Macro-F1 Score: {macro_f1:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "\n",
    "# --- Plot Confusion Matrix ---\n",
    "print(\"\\n--- Plotting Confusion Matrix ---\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a48236",
   "metadata": {},
   "source": [
    "# Explainability with SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf13db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SHAP explainer for the final model\n",
    "# Using a subset of the training data for the background is a common practice for performance\n",
    "explainer = shap.TreeExplainer(final_model, X_train_final.sample(100, random_state=42))\n",
    "shap_values = explainer.shap_values(X_test_final)\n",
    "\n",
    "# SHAP Summary Plot (Bar)\n",
    "print(\"\\nGenerating SHAP Summary Bar Plot...\")\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_test_final, plot_type=\"bar\", class_names=le.classes_, show=False)\n",
    "plt.title(\"SHAP Summary Plot (Global Feature Importance)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# SHAP Summary Plot (Beeswarm)\n",
    "print(\"\\nGenerating SHAP Summary Beeswarm Plot...\")\n",
    "plt.figure()\n",
    "shap.summary_plot(shap_values, X_test_final, class_names=le.classes_, show=False,  plot_type=\"violin\")\n",
    "plt.title(\"SHAP Summary Plot (Beeswarm)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "droughtmonitoringiran (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
