{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466fb799",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c141a822",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pooya/w/DroughtMonitoringIran/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import sqlite3\n",
    "import warnings\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, SMOTENC, BorderlineSMOTE, ADASYN\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier # Changed from ExtraTreesClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27845aff",
   "metadata": {},
   "source": [
    "# Load Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1361a6af",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append('/home/pooya/w/DroughtMonitoringIran/')\n",
    "\n",
    "DATABASE_PATH = \"./database/database.db\"\n",
    "\n",
    "RESULTS_DIR = 'results_smote'\n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "conn = sqlite3.connect(DATABASE_PATH)\n",
    "\n",
    "data = pd.read_sql(sql='SELECT * FROM data', con=conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7daed0",
   "metadata": {},
   "source": [
    "# Generate Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "239c059a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No classes with fewer than 2 samples found.\n",
      "Target Classes: ['ED' 'EW' 'MD' 'MW' 'NN' 'SD' 'VW']\n",
      "Training set shape: (668, 74)\n",
      "Testing set shape: (287, 74)\n",
      "Class distribution before SMOTE: {np.int64(0): np.int64(25), np.int64(1): np.int64(15), np.int64(2): np.int64(37), np.int64(3): np.int64(57), np.int64(4): np.int64(484), np.int64(5): np.int64(20), np.int64(6): np.int64(30)}\n",
      "Class distribution after SMOTE: {np.int64(0): np.int64(484), np.int64(1): np.int64(484), np.int64(2): np.int64(484), np.int64(3): np.int64(484), np.int64(4): np.int64(484), np.int64(5): np.int64(484), np.int64(6): np.int64(484)}\n",
      "Training set shape after SMOTE: (3388, 74)\n",
      "Optimal number of features found: 73\n",
      "Selected features: ['Station_Latitude', 'Station_Longitude', 'Station_Elevation', 'GPM_SPI_12', 'ERA5_SPI_12', 'ERA5_Precipitation', 'GPM_Precipitation', 'PET_MOD16A2GF', 'NDVI', 'EVI', 'LSTDay', 'LSTNight', 'LST', 'PCI_GPM', 'PCI_ERA5', 'VCI', 'TCI', 'VHI', 'CI_GPM', 'CI_ERA5', 'Month_sin', 'Month_cos', 'LST_Diff', 'ERA5_Precipitation_lag_1', 'GPM_Precipitation_lag_1', 'GPM_SPI_12_lag_1', 'ERA5_SPI_12_lag_1', 'PET_MOD16A2GF_lag_1', 'NDVI_lag_1', 'EVI_lag_1', 'LSTDay_lag_1', 'LSTNight_lag_1', 'LST_lag_1', 'PCI_GPM_lag_1', 'PCI_ERA5_lag_1', 'VCI_lag_1', 'TCI_lag_1', 'VHI_lag_1', 'CI_GPM_lag_1', 'CI_ERA5_lag_1', 'ERA5_Precipitation_lag_2', 'GPM_Precipitation_lag_2', 'GPM_SPI_12_lag_2', 'ERA5_SPI_12_lag_2', 'PET_MOD16A2GF_lag_2', 'NDVI_lag_2', 'EVI_lag_2', 'LSTDay_lag_2', 'LSTNight_lag_2', 'LST_lag_2', 'PCI_GPM_lag_2', 'PCI_ERA5_lag_2', 'VCI_lag_2', 'TCI_lag_2', 'VHI_lag_2', 'CI_GPM_lag_2', 'CI_ERA5_lag_2', 'ERA5_Precipitation_lag_3', 'GPM_Precipitation_lag_3', 'GPM_SPI_12_lag_3', 'ERA5_SPI_12_lag_3', 'PET_MOD16A2GF_lag_3', 'NDVI_lag_3', 'EVI_lag_3', 'LSTDay_lag_3', 'LSTNight_lag_3', 'LST_lag_3', 'PCI_GPM_lag_3', 'VCI_lag_3', 'TCI_lag_3', 'VHI_lag_3', 'CI_GPM_lag_3', 'CI_ERA5_lag_3']\n",
      "Selected features saved to results_smote/C2/SPI_12/selected_features.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-24 21:23:44,664] A new study created in memory with name: no-name-c63604c2-fdfc-4bc8-9da4-2bbd614378ee\n",
      "[I 2025-08-24 21:23:47,535] Trial 0 finished with value: 0.9015341080476352 and parameters: {'n_estimators': 170, 'max_depth': 24, 'min_samples_split': 19, 'min_samples_leaf': 18, 'max_features': 0.16464594939478006, 'criterion': 'gini'}. Best is trial 0 with value: 0.9015341080476352.\n",
      "[I 2025-08-24 21:24:06,875] Trial 1 finished with value: 0.9473496667695468 and parameters: {'n_estimators': 272, 'max_depth': 34, 'min_samples_split': 19, 'min_samples_leaf': 1, 'max_features': 0.7024607231096078, 'criterion': 'gini'}. Best is trial 1 with value: 0.9473496667695468.\n",
      "[I 2025-08-24 21:24:26,951] Trial 2 finished with value: 0.944890510806671 and parameters: {'n_estimators': 364, 'max_depth': 48, 'min_samples_split': 6, 'min_samples_leaf': 4, 'max_features': 0.815085587241152, 'criterion': 'gini'}. Best is trial 1 with value: 0.9473496667695468.\n",
      "[I 2025-08-24 21:24:32,409] Trial 3 finished with value: 0.9030131124328207 and parameters: {'n_estimators': 175, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 16, 'max_features': 0.32526355417597835, 'criterion': 'entropy'}. Best is trial 1 with value: 0.9473496667695468.\n",
      "[I 2025-08-24 21:24:41,379] Trial 4 finished with value: 0.8886740158758342 and parameters: {'n_estimators': 295, 'max_depth': 23, 'min_samples_split': 6, 'min_samples_leaf': 19, 'max_features': 0.538762324712654, 'criterion': 'gini'}. Best is trial 1 with value: 0.9473496667695468.\n",
      "[I 2025-08-24 21:24:45,412] Trial 5 finished with value: 0.9635922039870772 and parameters: {'n_estimators': 134, 'max_depth': 44, 'min_samples_split': 6, 'min_samples_leaf': 2, 'max_features': 0.34250093909378887, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:24:50,429] Trial 6 finished with value: 0.935949680117999 and parameters: {'n_estimators': 265, 'max_depth': 37, 'min_samples_split': 13, 'min_samples_leaf': 8, 'max_features': 0.1500671961566225, 'criterion': 'entropy'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:25:07,398] Trial 7 finished with value: 0.9187252314809061 and parameters: {'n_estimators': 429, 'max_depth': 11, 'min_samples_split': 4, 'min_samples_leaf': 12, 'max_features': 0.4363553390371311, 'criterion': 'entropy'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:25:10,685] Trial 8 finished with value: 0.9321604261709335 and parameters: {'n_estimators': 106, 'max_depth': 25, 'min_samples_split': 20, 'min_samples_leaf': 8, 'max_features': 0.39946263798917736, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:25:18,429] Trial 9 finished with value: 0.9003409485604623 and parameters: {'n_estimators': 126, 'max_depth': 32, 'min_samples_split': 19, 'min_samples_leaf': 18, 'max_features': 0.1480850733069291, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:25:23,954] Trial 10 finished with value: 0.9490586885812631 and parameters: {'n_estimators': 51, 'max_depth': 50, 'min_samples_split': 11, 'min_samples_leaf': 1, 'max_features': 0.9592560665213737, 'criterion': 'entropy'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:25:29,994] Trial 11 finished with value: 0.9490388046529784 and parameters: {'n_estimators': 58, 'max_depth': 50, 'min_samples_split': 12, 'min_samples_leaf': 1, 'max_features': 0.9495763568979337, 'criterion': 'entropy'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:25:36,296] Trial 12 finished with value: 0.9436020976012547 and parameters: {'n_estimators': 84, 'max_depth': 43, 'min_samples_split': 9, 'min_samples_leaf': 5, 'max_features': 0.6794162807732115, 'criterion': 'entropy'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:25:56,268] Trial 13 finished with value: 0.9440773127041002 and parameters: {'n_estimators': 175, 'max_depth': 41, 'min_samples_split': 9, 'min_samples_leaf': 4, 'max_features': 0.9719770565402771, 'criterion': 'entropy'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:25:57,736] Trial 14 finished with value: 0.9159573755278052 and parameters: {'n_estimators': 59, 'max_depth': 44, 'min_samples_split': 14, 'min_samples_leaf': 13, 'max_features': 0.29873353650521306, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:26:09,793] Trial 15 finished with value: 0.9380580245353876 and parameters: {'n_estimators': 227, 'max_depth': 49, 'min_samples_split': 2, 'min_samples_leaf': 7, 'max_features': 0.5594188592489233, 'criterion': 'entropy'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:26:18,320] Trial 16 finished with value: 0.9515340515034116 and parameters: {'n_estimators': 138, 'max_depth': 18, 'min_samples_split': 9, 'min_samples_leaf': 1, 'max_features': 0.8289533691602434, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:26:45,780] Trial 17 finished with value: 0.9467042769084939 and parameters: {'n_estimators': 486, 'max_depth': 19, 'min_samples_split': 9, 'min_samples_leaf': 3, 'max_features': 0.8240363669286761, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:26:49,824] Trial 18 finished with value: 0.8473331615199161 and parameters: {'n_estimators': 145, 'max_depth': 5, 'min_samples_split': 7, 'min_samples_leaf': 6, 'max_features': 0.5765538767765459, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:26:54,236] Trial 19 finished with value: 0.9302853848929328 and parameters: {'n_estimators': 223, 'max_depth': 18, 'min_samples_split': 15, 'min_samples_leaf': 10, 'max_features': 0.2436648238174457, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:27:12,567] Trial 20 finished with value: 0.9481745040280727 and parameters: {'n_estimators': 363, 'max_depth': 30, 'min_samples_split': 16, 'min_samples_leaf': 3, 'max_features': 0.4530745430079085, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:27:21,948] Trial 21 finished with value: 0.9503628405084543 and parameters: {'n_estimators': 106, 'max_depth': 39, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 0.8782999010781757, 'criterion': 'entropy'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:27:35,397] Trial 22 finished with value: 0.9469381855702753 and parameters: {'n_estimators': 216, 'max_depth': 38, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 0.8582642928891897, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:27:42,632] Trial 23 finished with value: 0.9451125507094462 and parameters: {'n_estimators': 99, 'max_depth': 17, 'min_samples_split': 7, 'min_samples_leaf': 5, 'max_features': 0.7162376673970212, 'criterion': 'entropy'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:27:54,741] Trial 24 finished with value: 0.9443262618264718 and parameters: {'n_estimators': 144, 'max_depth': 37, 'min_samples_split': 8, 'min_samples_leaf': 3, 'max_features': 0.8837060745261618, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:28:02,363] Trial 25 finished with value: 0.9201018639124949 and parameters: {'n_estimators': 191, 'max_depth': 45, 'min_samples_split': 4, 'min_samples_leaf': 9, 'max_features': 0.6400263451052388, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:28:11,413] Trial 26 finished with value: 0.9354274769125578 and parameters: {'n_estimators': 125, 'max_depth': 28, 'min_samples_split': 12, 'min_samples_leaf': 6, 'max_features': 0.7629629436682522, 'criterion': 'entropy'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:28:17,352] Trial 27 finished with value: 0.9472330787236383 and parameters: {'n_estimators': 88, 'max_depth': 40, 'min_samples_split': 2, 'min_samples_leaf': 2, 'max_features': 0.9037306385181084, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:28:26,689] Trial 28 finished with value: 0.9023658815425939 and parameters: {'n_estimators': 144, 'max_depth': 11, 'min_samples_split': 11, 'min_samples_leaf': 14, 'max_features': 0.7671483873449931, 'criterion': 'entropy'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:28:32,991] Trial 29 finished with value: 0.9561124609560305 and parameters: {'n_estimators': 320, 'max_depth': 34, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_features': 0.22041937211157747, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:28:39,870] Trial 30 finished with value: 0.9502517703554438 and parameters: {'n_estimators': 348, 'max_depth': 34, 'min_samples_split': 5, 'min_samples_leaf': 5, 'max_features': 0.2247225757200806, 'criterion': 'gini'}. Best is trial 5 with value: 0.9635922039870772.\n",
      "[I 2025-08-24 21:28:48,511] Trial 31 finished with value: 0.968794571612567 and parameters: {'n_estimators': 318, 'max_depth': 46, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 0.32084110904643454, 'criterion': 'gini'}. Best is trial 31 with value: 0.968794571612567.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters saved to results_smote/C2/SPI_12/best_params.json\n",
      "Best trial found: 31\n",
      "Best Macro-F1 Score: 0.968794571612567\n",
      "Best Hyperparameters:\n",
      "{'n_estimators': 318, 'max_depth': 46, 'min_samples_split': 7, 'min_samples_leaf': 1, 'max_features': 0.32084110904643454, 'criterion': 'gini'}\n",
      "Evaluation metrics saved to results_smote/C2/SPI_12/evaluation_metrics.txt\n",
      "Overall Accuracy: 0.7247\n",
      "Macro-F1 Score: 0.4660\n",
      "Cohen's Kappa: 0.3528\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ED       0.70      0.64      0.67        11\n",
      "          EW       1.00      0.67      0.80         6\n",
      "          MD       0.11      0.06      0.08        16\n",
      "          MW       0.18      0.12      0.14        25\n",
      "          NN       0.83      0.89      0.86       208\n",
      "          SD       0.33      0.44      0.38         9\n",
      "          VW       0.33      0.33      0.33        12\n",
      "\n",
      "    accuracy                           0.72       287\n",
      "   macro avg       0.50      0.45      0.47       287\n",
      "weighted avg       0.69      0.72      0.71       287\n",
      "\n",
      "\n",
      "--- Plotting Confusion Matrix ---\n"
     ]
    }
   ],
   "source": [
    "def run(di, di_scale, selected_stations, stations_group_name, start_date, end_date):\n",
    "    \n",
    "    group_dir = os.path.join(RESULTS_DIR, stations_group_name)\n",
    "    os.makedirs(group_dir, exist_ok=True)\n",
    "    \n",
    "    scale_dir = os.path.join(group_dir, f'{di}_{di_scale}')\n",
    "    os.makedirs(scale_dir, exist_ok=True)\n",
    "    \n",
    "    # Select Columns\n",
    "    selected_columns = [\n",
    "        'Station_Name', 'Station_ID',\n",
    "        'Station_Latitude', 'Station_Longitude', 'Station_Elevation',\n",
    "        'Date',\n",
    "        f'{di}_{di_scale}',\n",
    "        f'GPM_{di}_{di_scale}',\n",
    "        f'ERA5_{di}_{di_scale}',\n",
    "        'ERA5_Precipitation',\n",
    "        'GPM_Precipitation',\n",
    "        'PET_MOD16A2GF',\n",
    "        'NDVI', 'EVI',\n",
    "        'LSTDay', 'LSTNight', 'LST',\n",
    "        'PCI_GPM', 'PCI_ERA5',\n",
    "        'VCI', 'TCI', 'VHI',\n",
    "        'CI_GPM', 'CI_ERA5',\n",
    "    ]\n",
    "    \n",
    "    df = data\\\n",
    "    .filter(items=selected_columns)\\\n",
    "        .query(\"Station_Name in @selected_stations and Date >= @start_date and Date < @end_date\")\n",
    "\n",
    "    # Date, Year, Month\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m')\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "\n",
    "    # SPI and SPEI Class\n",
    "    df[f'{di}_{di_scale}_Class'] = pd.cut(df[f'{di}_{di_scale}'], bins=[-10, -2, -1.5, -1, 1, 1.5, 2, 10], labels=['ED', 'SD', 'MD', 'NN', 'MW', 'VW', 'EW'])\n",
    "\n",
    "    # Month Sin & Cos\n",
    "    df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "    df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "            \n",
    "    # LST Diff\n",
    "    df['LST_Diff'] = df['LSTDay'] - df['LSTNight']\n",
    "\n",
    "    # Convert to Category\n",
    "    df['Station_ID'] = df['Station_ID'].astype('category')\n",
    "    df['Year'] = df['Year'].astype('category')\n",
    "    df['Month'] = df['Month'].astype('category')\n",
    "    df[f'{di}_{di_scale}_Class'] = df[f'{di}_{di_scale}_Class'].astype('category')\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    df.sort_values(by=['Station_ID', 'Year', 'Month'], inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    selected_columns_lag_roll = [\n",
    "        'ERA5_Precipitation',\n",
    "        'GPM_Precipitation',\n",
    "        f'GPM_{di}_{di_scale}',\n",
    "        f'ERA5_{di}_{di_scale}',\n",
    "        'PET_MOD16A2GF',\n",
    "        'NDVI', 'EVI',\n",
    "        'LSTDay', 'LSTNight', 'LST',\n",
    "        'PCI_GPM', 'PCI_ERA5',\n",
    "        'VCI', 'TCI', 'VHI',\n",
    "        'CI_GPM', 'CI_ERA5',\n",
    "    ]\n",
    "\n",
    "    # Add Lag\n",
    "    for lag in range(1, 4):\n",
    "        for col in selected_columns_lag_roll:\n",
    "            df[f'{col}_lag_{lag}'] = df.groupby('Station_ID', observed=False)[col].shift(lag)\n",
    "\n",
    "\n",
    "    # # Add Mean and Std Roll\n",
    "    # for r in [3, 6]:\n",
    "    #     for col in selected_columns_lag_roll:\n",
    "    #         df[f'{col}_roll_mean_{r}'] = df.groupby('Station_ID', observed=False)[col].transform(lambda x: x.rolling(window=r, min_periods=1).mean())\n",
    "    #         df[f'{col}_roll_std_{r}'] = df.groupby('Station_ID', observed=False)[col].transform(lambda x: x.rolling(window=r, min_periods=1).std())\n",
    "    \n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Define columns to exclude from features\n",
    "    EXCLUDE_COLS = ['Station_Name', 'Station_ID', 'Date', f'{di}_{di_scale}_Class', f'{di}_{di_scale}', 'Year', 'Month']\n",
    "    FEATURES = [col for col in df.columns if col not in EXCLUDE_COLS]\n",
    "\n",
    "    # Station-wise standardization\n",
    "    df_scaled = df.copy()\n",
    "    for station in df_scaled['Station_Name'].unique():\n",
    "        station_mask = df_scaled['Station_Name'] == station\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled.loc[station_mask, FEATURES] = scaler.fit_transform(df_scaled.loc[station_mask, FEATURES])\n",
    "\n",
    "\n",
    "    class_counts = df_scaled[f'{di}_{di_scale}_Class'].value_counts()\n",
    "    classes_to_remove = class_counts[class_counts < 2].index\n",
    "\n",
    "    if not classes_to_remove.empty:\n",
    "        print(f\"Removing classes with fewer than 2 samples: {classes_to_remove.tolist()}\")\n",
    "        \n",
    "        # Filter the dataframe to exclude these rare classes\n",
    "        df_filtered = df_scaled[~df_scaled[f'{di}_{di_scale}_Class'].isin(classes_to_remove)].copy()\n",
    "        \n",
    "        # Redefine X and y from the filtered dataframe\n",
    "        X = df_filtered[FEATURES]\n",
    "        y = df_filtered[f'{di}_{di_scale}_Class']\n",
    "        \n",
    "        print(f\"Data shape after removing rare classes: {X.shape}\")\n",
    "        print(\"\\n--- Final Class Distribution ---\")\n",
    "        print(y.value_counts())\n",
    "    else:\n",
    "        print(\"No classes with fewer than 2 samples found.\")\n",
    "        X = df_scaled[FEATURES]\n",
    "        y = df_scaled[f'{di}_{di_scale}_Class']\n",
    "\n",
    "    # Encode the target variable\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X,\n",
    "        y_encoded,\n",
    "        test_size=0.30,\n",
    "        random_state=42,\n",
    "        stratify=y_encoded\n",
    "    )\n",
    "\n",
    "    print(f\"Target Classes: {le.classes_}\")\n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Testing set shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "    # Check class distribution before SMOTE\n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    print(f\"Class distribution before SMOTE: {dict(zip(unique, counts))}\")\n",
    "    \n",
    "    # Apply SMOTE to balance the training data\n",
    "    smote = BorderlineSMOTE(random_state=42)\n",
    "    # smote = ADASYN(random_state=42)\n",
    "    \n",
    "    try:\n",
    "        X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "        \n",
    "        # Check class distribution after SMOTE\n",
    "        unique_smote, counts_smote = np.unique(y_train_smote, return_counts=True)\n",
    "        print(f\"Class distribution after SMOTE: {dict(zip(unique_smote, counts_smote))}\")\n",
    "        print(f\"Training set shape after SMOTE: {X_train_smote.shape}\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"SMOTE failed: {e}\")\n",
    "        print(\"Using original training data without SMOTE\")\n",
    "        X_train_smote, y_train_smote = X_train, y_train\n",
    "\n",
    "\n",
    "    # The estimator that will be used by RFECV\n",
    "    estimator = RandomForestClassifier(\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # The RFECV object\n",
    "    rfecv = RFECV(\n",
    "        estimator=estimator,\n",
    "        step=1,\n",
    "        cv=StratifiedKFold(5),\n",
    "        scoring='f1_macro',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Fit RFECV on the training data\n",
    "    rfecv.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "    print(f\"Optimal number of features found: {rfecv.n_features_}\")\n",
    "\n",
    "    # Get the final selected features\n",
    "    final_features = X_train_smote.columns[rfecv.support_]\n",
    "    print(f\"Selected features: {final_features.tolist()}\")\n",
    "    \n",
    "    features_path = os.path.join(scale_dir, 'selected_features.txt')\n",
    "    with open(features_path, 'w') as f:\n",
    "        for feature in final_features:\n",
    "            f.write(f\"{feature}\\n\")\n",
    "    print(f\"Selected features saved to {features_path}\")\n",
    "\n",
    "    # --- Plot Feature Importances ---\n",
    "    # The `rfecv.estimator_` attribute is the model trained on the full set of features\n",
    "    # during the last step of the cross-validation process. We can use its feature importances.\n",
    "    importances = rfecv.estimator_.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'Feature': final_features,\n",
    "        'Importance': importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    # Plot only the selected features\n",
    "    selected_feature_importances = feature_importance_df[feature_importance_df['Feature'].isin(final_features)]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=selected_feature_importances)\n",
    "    plt.title('Feature Importances for Selected Features (from RFECV)')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.tight_layout()\n",
    "    fig_path = os.path.join(scale_dir, 'feature_importances.png')\n",
    "    plt.savefig(fig_path)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # Update training and testing sets with selected features\n",
    "    X_train_final = X_train_smote[final_features]\n",
    "    X_test_final = X_test[final_features]\n",
    "    y_train_final = y_train_smote\n",
    "    \n",
    "    \n",
    "    def objective(trial):\n",
    "        \"\"\"Define the objective function for Optuna to optimize.\"\"\"\n",
    "        params = {\n",
    "            'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "            'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "            'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "            'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "            'max_features': trial.suggest_float('max_features', 0.1, 1.0),\n",
    "            'criterion': trial.suggest_categorical('criterion', ['gini', 'entropy']),\n",
    "            'random_state': 42,\n",
    "            'n_jobs': -1\n",
    "        }\n",
    "        \n",
    "        model = RandomForestClassifier(**params)\n",
    "        \n",
    "        # Stratified K-Fold Cross-Validation\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        f1_scores = []\n",
    "        \n",
    "        for train_idx, val_idx in skf.split(X_train_final, y_train_final):\n",
    "            X_train_fold, X_val_fold = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y_train_final[train_idx], y_train_final[val_idx]\n",
    "            \n",
    "            model.fit(X_train_fold, y_train_fold)\n",
    "            preds = model.predict(X_val_fold)\n",
    "            f1_scores.append(f1_score(y_val_fold, preds, average='macro'))\n",
    "            \n",
    "        return np.mean(f1_scores)\n",
    "\n",
    "    # Create a study object and optimize\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=50, timeout=300) # 50 trials or 5 minutes\n",
    "\n",
    "    # Save best parameters\n",
    "    best_params = study.best_params \n",
    "    params_path = os.path.join(scale_dir, 'best_params.json')\n",
    "    with open(params_path, 'w') as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "    print(f\"Best parameters saved to {params_path}\")\n",
    "\n",
    "    print(f\"Best trial found: {study.best_trial.number}\")\n",
    "    print(f\"Best Macro-F1 Score: {study.best_value}\")\n",
    "    print(\"Best Hyperparameters:\")\n",
    "    print(study.best_params)\n",
    "\n",
    "\n",
    "\n",
    "    # Get best hyperparameters\n",
    "    best_params = study.best_params\n",
    "    best_params['random_state'] = 42\n",
    "    best_params['n_jobs'] = -1\n",
    "\n",
    "    # Train the final model with SMOTE data\n",
    "    final_model = RandomForestClassifier(**best_params)\n",
    "    final_model.fit(X_train_final, y_train_final)\n",
    "\n",
    "    # Make predictions on the test set (original test set, not SMOTE)\n",
    "    y_pred = final_model.predict(X_test_final)\n",
    "\n",
    "    # Save evaluation metrics\n",
    "    report = classification_report(y_test, y_pred, target_names=le.classes_)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    \n",
    "    metrics_path = os.path.join(scale_dir, 'evaluation_metrics.txt')\n",
    "    with open(metrics_path, 'w') as f:\n",
    "        f.write(f\"Overall Accuracy: {accuracy:.4f}\\n\")\n",
    "        f.write(f\"Macro-F1 Score: {macro_f1:.4f}\\n\")\n",
    "        f.write(f\"Cohen's Kappa: {kappa:.4f}\\n\\n\")\n",
    "        f.write(\"Classification Report:\\n\")\n",
    "        f.write(report)\n",
    "    print(f\"Evaluation metrics saved to {metrics_path}\")\n",
    "\n",
    "    print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro-F1 Score: {macro_f1:.4f}\")\n",
    "    print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "\n",
    "    # --- Plot Confusion Matrix ---\n",
    "    print(\"\\n--- Plotting Confusion Matrix ---\")\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predicted')\n",
    "    fig_path = os.path.join(scale_dir, 'confusion_matrix.png')\n",
    "    plt.savefig(fig_path)\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# for di in ['SPI', 'SPEI']:\n",
    "for di in ['SPI']:\n",
    "    # for di_scale in [1, 3, 6, 9, 12, 18, 24]:\n",
    "    for di_scale in [12]:\n",
    "        # for stations_group_name in ['ALL', 'C1', 'C2', 'C3']: \n",
    "        for stations_group_name in ['C2']: \n",
    "\n",
    "            if stations_group_name == 'ALL':\n",
    "                selected_stations = ['Ramsar', 'Nowshahr', 'Siahbisheh', 'Kiyasar', 'Kojur', 'Baladeh', 'Alasht', 'Babolsar', 'Gharakhil', 'Sari', 'Sari (dasht-e-naz airport)', 'Galugah', 'Bandar-e-amirabad', 'Amol', 'Polsefid']\n",
    "            elif stations_group_name == 'C3':\n",
    "                selected_stations = ['Ramsar', 'Nowshahr']\n",
    "            elif stations_group_name == 'C2':\n",
    "                selected_stations = ['Siahbisheh', 'Kiyasar', 'Kojur', 'Baladeh', 'Alasht']\n",
    "            elif stations_group_name == 'C1':\n",
    "                selected_stations = ['Babolsar', 'Gharakhil', 'Sari', 'Sari (dasht-e-naz airport)', 'Galugah', 'Bandar-e-amirabad', 'Amol', 'Polsefid']\n",
    "            else:\n",
    "                selected_stations = ['Ramsar', 'Nowshahr', 'Siahbisheh', 'Kiyasar', 'Kojur', 'Baladeh', 'Alasht', 'Babolsar', 'Gharakhil', 'Sari', 'Sari (dasht-e-naz airport)', 'Galugah', 'Bandar-e-amirabad', 'Amol', 'Polsefid']\n",
    "\n",
    "            ex_dir = os.path.join(RESULTS_DIR, stations_group_name, f'{di}_{di_scale}')\n",
    "            if os.path.exists(ex_dir):\n",
    "                continue \n",
    "            \n",
    "            run(\n",
    "                di=di,\n",
    "                di_scale=di_scale,\n",
    "                selected_stations=selected_stations,\n",
    "                stations_group_name=stations_group_name,\n",
    "                start_date='2006-09', \n",
    "                end_date='2023-10'\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "droughtmonitoringiran (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
